# Roadmap Optimis√© pour Algorithmes G√©n√©tiques - Seapopym

## üéØ **Contexte d'Usage Critique**

**Workflow d'optimisation** :
```
Algorithme G√©n√©tique (externe) 
    ‚Üì g√©n√®re N jeux de param√®tres
    ‚Üì pour chaque param√®tre_set:
        ‚Üí Cr√©er mod√®le Seapopym(param√®tre_set, for√ßages_fixes)
        ‚Üí Calculer biomasse_finale
        ‚Üí Extraire score fitness
    ‚Üì r√©p√©ter sur plusieurs g√©n√©rations
```

**Contraintes critiques identifi√©es** :
- **M√™mes for√ßages** r√©utilis√©s pour tous les mod√®les (optimisation possible)
- **Cr√©ation/destruction r√©p√©t√©e** de mod√®les (risque memory leak)
- **Score bas√© sur biomasse** (performance calcul final critique)
- **Volume √©lev√©** simulations (centaines/milliers par g√©n√©ration)

---

## üö® **Points Critiques √† Absolument Traiter**

### **1. Memory Leak Prevention (CRITIQUE ‚≠ê‚≠ê‚≠ê)**

**Probl√®me** : Cr√©ation r√©p√©t√©e de mod√®les ‚Üí accumulation progressive m√©moire
```python
# DANGER - Pattern actuel probablement utilis√©
for param_set in genetic_population:
    model = NoTransportModel(param_set, forcings)  # New instance each time
    result = model.run()                           # Potentially keeps references
    score = extract_fitness(result.biomass)       # Large arrays in memory
    # model goes out of scope but may not be freed immediately
```

**Solution obligatoire** : Context managers + explicit cleanup
```python
# SAFE - Pattern √† impl√©menter
@contextmanager
def genetic_simulation(param_set, shared_forcings):
    model = create_optimized_model(param_set, shared_forcings)
    try:
        yield model
    finally:
        model.cleanup()  # Explicit resource release
        gc.collect()     # Force garbage collection

for param_set in genetic_population:
    with genetic_simulation(param_set, forcings) as model:
        result = model.run()
        score = extract_fitness(result.biomass)
        # Auto-cleanup guaranteed
```

### **2. Shared Forcings Optimization (PERFORMANCE ‚≠ê‚≠ê‚≠ê)**

**Opportunit√© majeure** : For√ßages identiques ‚Üí chargement unique
```python
# BEFORE - Inefficient
for param_set in population:
    model = Model(param_set)
    model.load_forcings(forcing_files)  # Reload same data every time
    
# AFTER - Optimized  
shared_forcings = PreloadedForcings(forcing_files)  # Load once
for param_set in population:
    model = Model(param_set, forcings=shared_forcings)  # Reference shared data
```

**Gains attendus** : 10x faster model initialization + 90% moins m√©moire

### **3. Biomass Extraction Performance (CRITIQUE ‚≠ê‚≠ê‚≠ê)**

**Goulot d'√©tranglement probable** : Calcul/extraction finale biomasse
```python
# Optimisations essentielles
- apply_ufunc pour calcul biomasse (2-3x speedup)
- Extraction s√©lective (√©viter copy full datasets)
- Compiled functions pour m√©triques fitness
- Cache interm√©diaires si possible
```

---

## üìã **Roadmap R√©vis√© pour Usage G√©n√©tique**

### **Phase 0 : Genetic Algorithm Profiling** ‚≠ê‚≠ê‚≠ê (1 semaine)

**T√¢che 0.1 : Workflow Analysis**
```python
# Profiler EXACTEMENT votre workflow actuel
- Temps par simulation individuelle
- Memory usage pattern over generations  
- Identifier les 3 plus gros bottlenecks
- Mesurer overhead model creation/destruction
```

**T√¢che 0.2 : Shared Resources Identification**
```python
# Cataloguer tout ce qui peut √™tre partag√©
- Forcings data (obviament)
- Coordinate definitions
- Template structures
- Compiled functions
- Configuration objects immuables
```

**T√¢che 0.3 : Memory Pattern Baseline**
```python
# Test actuel sur mini genetic run
def profile_genetic_memory():
    memory_samples = []
    for generation in range(5):
        for individual in small_population:
            mem_before = psutil.virtual_memory().used
            score = run_simulation(individual)
            mem_after = psutil.virtual_memory().used
            memory_samples.append(mem_after - mem_before)
    return analyze_memory_pattern(memory_samples)
```

### **Phase 1 : Genetic-Optimized Architecture** ‚≠ê‚≠ê‚≠ê (2 semaines)

**T√¢che 1.1 : Shared Resource Manager**
```python
class GeneticResourceManager:
    def __init__(self, forcing_files):
        self.forcings = self._load_shared_forcings(forcing_files)
        self.templates = self._precompile_templates()
        self.coords = self._setup_coordinates()
    
    def create_model(self, params) -> SeapopymModel:
        """Fast model creation using shared resources."""
        return SeapopymModel(
            parameters=params,
            forcings=self.forcings,        # Shared reference
            templates=self.templates,      # Pre-built
            coordinates=self.coords        # Reused
        )
```

**T√¢che 1.2 : Memory-Safe Simulation Context**
```python
@contextmanager  
def genetic_individual_simulation(resource_manager, params):
    model = resource_manager.create_model(params)
    try:
        yield model
    finally:
        model._clear_computation_cache()  # Clean intermediate results
        del model  # Explicit deletion
        # Note: shared resources remain in memory
```

**T√¢che 1.3 : Fast Fitness Extraction**
```python
def extract_fitness_optimized(seapopym_result):
    """Ultra-fast biomass ‚Üí fitness conversion."""
    # Use apply_ufunc for vectorized operations
    # Avoid unnecessary data copying
    # Return scalar/small array only
    return compiled_fitness_function(seapopym_result.biomass)
```

### **Phase 2 : Performance Critical Path** ‚≠ê‚≠ê‚≠ê (2 semaines)

**T√¢che 2.1 : apply_ufunc Priority Migration**
```python
# Focus UNIQUEMENT sur functions utilis√©es dans biomass calc
Priority_Order = [
    "biomass.py",           # Final computation
    "production.py",        # Core calculation  
    "mortality_field.py",   # Parameter-sensitive
]
# Objectif: 2x speedup minimum sur ces 3 fonctions
```

**T√¢che 2.2 : Compiled Fitness Functions**
```python
@numba.jit(nopython=True)
def compiled_fitness_metrics(biomass_array):
    """JIT-compiled fitness calculations."""
    # Ultra-fast biomass processing
    # Custom metrics optimized for your GA
    return fitness_score

# Integration with seapopym
def fast_fitness_extraction(seapopym_state):
    biomass = seapopym_state[ForcingLabels.biomass].values
    return compiled_fitness_metrics(biomass)
```

**T√¢che 2.3 : Kernel Chain Optimization**
```python
# Optimize kernel execution for repeated similar runs
class GeneticOptimizedKernel:
    def __init__(self):
        self._cached_intermediates = {}
        
    def run(self, params, shared_forcings):
        # Cache invariant computations
        # Skip redundant calculations
        # Optimize for parameter-only variations
```

### **Phase 3 : Genetic Algorithm Integration** ‚≠ê‚≠ê (1 semaine)

**T√¢che 3.1 : High-Level GA Interface**
```python
class SeapopymGeneticOptimizer:
    def __init__(self, forcing_files, model_type="no_transport"):
        self.resources = GeneticResourceManager(forcing_files)
        
    def evaluate_population(self, parameter_population):
        """Evaluate entire GA population efficiently."""
        scores = []
        for params in parameter_population:
            with genetic_individual_simulation(self.resources, params) as model:
                result = model.run()
                score = extract_fitness_optimized(result)
                scores.append(score)
        return np.array(scores)
        
    def cleanup_generation(self):
        """Call between generations to prevent memory accumulation."""
        gc.collect()
        self.resources.clear_caches()
```

**T√¢che 3.2 : Memory Monitoring Integration**
```python
# Protection automatique contre explosion m√©moire
def monitored_genetic_evaluation(population, max_memory_gb=4):
    memory_samples = []
    
    for i, params in enumerate(population):
        if psutil.virtual_memory().used > max_memory_gb * GB:
            warning(f"Memory limit approaching at individual {i}")
            gc.collect()  # Emergency cleanup
            
        score = evaluate_individual(params)
        memory_samples.append(psutil.virtual_memory().used)
    
    return scores, memory_samples
```

### **Phase 4 : Validation & Benchmarks** ‚≠ê‚≠ê (1 semaine)

**T√¢che 4.1 : Genetic Algorithm Stress Test**
```python
def test_genetic_scalability():
    # Simulate realistic GA run
    populations = [50, 100, 200]  # Typical GA sizes
    generations = [10, 20, 50]    # Duration tests
    
    for pop_size in populations:
        for gen_count in generations:
            memory_peak = run_genetic_simulation(pop_size, gen_count)
            assert memory_peak < MEMORY_LIMIT
            
def test_no_memory_accumulation():
    # Critical: memory should be stable across generations
    memory_per_generation = []
    for gen in range(20):
        mem_before = get_memory_usage()
        run_single_generation(population_size=100)
        mem_after = get_memory_usage()
        memory_per_generation.append(mem_after)
    
    # Memory should NOT grow linearly with generations
    assert np.std(memory_per_generation) < ACCEPTABLE_VARIANCE
```

---

## üéØ **M√©triques de Succ√®s Sp√©cifiques**

### **Performance Targets**
```python
# Baselines √† atteindre (ajustables selon votre hardware)
TARGET_METRICS = {
    "simulation_time": "<10s per individual",      # Actuel probablement 30-60s
    "memory_per_simulation": "<256MB",             # √âviter accumulation
    "population_100_time": "<15min total",        # Population enti√®re
    "memory_stability": "<5% variance g√©n√©rations", # Pas de leak
    "initialization_speedup": ">5x faster",       # Shared forcings
    "biomass_extraction": ">3x faster",           # apply_ufunc
}
```

### **Validation Checklist**
- [ ] Aucune accumulation m√©moire sur 50 g√©n√©rations
- [ ] Shared forcings correctement r√©utilis√©s
- [ ] Temps simulation lin√©aire avec taille population  
- [ ] Results identiques avant/apr√®s optimisation
- [ ] API simple pour int√©gration GA externe
- [ ] Monitoring automatique ressources
- [ ] Recovery graceful en cas probl√®me m√©moire

---

## üöÄ **Quick Wins Prioritaires**

**Impl√©mentation imm√©diate recommand√©e** :

1. **Context manager pour simulations** (2-3h dev)
2. **Shared forcings preloading** (1 jour dev)  
3. **Memory monitoring hooks** (0.5 jour dev)
4. **Fast fitness extraction** (1 jour dev)

Ces 4 points couvriront 80% de vos gains performance/m√©moire !

---

## ‚ö†Ô∏è **Pi√®ges √† √âviter Absolument**

1. **Ne pas optimiser l'architecture kernel/template** ‚Üí Elle est d√©j√† excellente
2. **Ne pas parall√©liser les simulations** ‚Üí GA externe g√®re d√©j√† √ßa
3. **Ne pas complexifier l'API** ‚Üí Garder interface simple
4. **Ne pas toucher aux calculs scientifiques** ‚Üí Focus sur infrastructure

**Principe directeur** : Optimiser l'**infrastructure d'ex√©cution**, pas la **science computationnelle** qui est d√©j√† au top ! üéØ